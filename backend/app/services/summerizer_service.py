from app.utils.embedding_generator import get_embeddings_model
from app.utils.vector_store import load_faiss_index
from app.utils.prompt_templates import get_summary_prompt
from app.utils.rag_chain_builder import create_rag_chain
from app.llms.gemini import get_gemini_llm
from app.llms.ollama import get_ollama_llm
from app.services.pdf_service import save_pdf_summary
import logging

logger = logging.getLogger(__name__)

async def generate_rag_summary(model_choice: str, title: str, owner: str):
    try:
        embeddings = get_embeddings_model()
        vector_store = load_faiss_index(f"faiss_indexes/{title}", embeddings)
        prompt = get_summary_prompt()

        rag_chain_gemini = create_rag_chain(get_gemini_llm(), vector_store, prompt)
        rag_chain_ollama = create_rag_chain(get_ollama_llm(), vector_store, prompt)

        if model_choice == "gemini":
            summary = rag_chain_gemini.invoke({"query": "Generate a structured summary."})
            logger.info("Summary generated by Gemini.")
            save_pdf_summary(title, summary, "gemini", owner)
            logger.info("Summary saved to database.")
            return summary

        elif model_choice == "ollama":
            summary = rag_chain_ollama.invoke({"query": "Generate a structured summary."})
            logger.info("Summary generated by Ollama.")
            save_pdf_summary(title, summary, "ollama", owner)
            logger.info("Summary saved to database.")
            return summary

        elif model_choice == "gemini_ollama":
            g_summary = rag_chain_gemini.invoke({"query": "Generate a structured summary."})
            o_summary = rag_chain_ollama.invoke({"query": "Generate a structured summary."})

            refinement_prompt = (
                "You are a research assistant. Refine the following two summaries into a single, accurate, and coherent academic summary. "
                "Format your response under the following headers:\n\n"
                "**Title & Authors:**\n"
                "Extract or infer the title and authors if available.\n\n"
                "**Abstract:**\n"
                "Provide a brief overview of the paper.\n\n"
                "**Problem Statement:**\n"
                "Clearly explain the main research problem or question being addressed.\n\n"
                "**Methodology:**\n"
                "Describe the techniques, methods, or approaches used in the research.\n\n"
                "**Key Results:**\n"
                "Summarize the most important findings and outcomes.\n\n"
                "**Conclusion:**\n"
                "Describe the final conclusion, implications, and possible future work.\n\n"
                "Use academic language. Be objective, concise, and do not add information that is not present in the summaries.\n\n"
                "Summary from Gemini:\n"
                f"{g_summary}\n\n"
                "Summary from Ollama:\n"
                f"{o_summary}\n\n"
                "Please refine and generate the final summary below:"
            )

            refined_summary = get_gemini_llm().invoke(refinement_prompt).content
            logger.info("Refined summary generated.")

            save_pdf_summary(title, refined_summary, "gemini_ollama", owner)
            logger.info("Refined summary saved to database.")
            return refined_summary

        else:
            logger.error(f"Invalid model choice provided: {model_choice}")
            raise ValueError(f"Invalid model choice: {model_choice}")

    except Exception as e:
        logger.exception(f"Exception during RAG summary generation: {e}")
        raise